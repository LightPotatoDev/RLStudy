{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 3.1 벨만 방정식 도출"
      ],
      "metadata": {
        "id": "Ur6PkR7uXoAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[확률 용어 정리]\n",
        "\n",
        "기댓값: 값 * 그 값이 발생할 확률의 합\n",
        "\n",
        "$E[x] = Σ xp(x)$\n",
        "\n",
        "조건부 확률: x가 일어났다는 가정 하에 y가 일어날 확률\n",
        "\n",
        "$p(y|x)$\n",
        "\n",
        "동시 확률: x와 y가 동시에 일어날 확률\n",
        "\n",
        "$p(x,y) = p(x)p(y|x)$\n",
        "\n",
        "보상의 기댓값: (x,y)일 때의 보상 * (x,y)가 동시에 일어날 확률\n",
        "\n",
        "$E[r(x,y)] = Σ_xΣ_yr(x,y)p(x,y)$"
      ],
      "metadata": {
        "id": "ntfzxjzcYBO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "수익: $G_t = R_t + \\gamma R_{t+1} + \\gamma^2R_{t+2} + ... (0 \\le \\gamma ≤ 1)$\n",
        "\n",
        "지속적 과제를 가정함<br>\n",
        "수익 $G_t$는 시간 t 이후로 얻을 수 있는 보상의 총합\n",
        "\n",
        "$G_t = R_t + \\gamma G_{t+1}$\n",
        "\n",
        "상태 가치 함수: 수익에 대한 기댓값\n",
        "\n",
        "$v_{\\pi}(s)\n",
        "\\\\= E_{\\pi}[G_t | S_t = s]\n",
        "\\\\= E_{\\pi}[G_{t+1} | S_{t+1} = s]\n",
        "\\\\= E_{\\pi}[R_t|S_t=s] + \\gamma E_{\\pi}[G_{t+1}|S_t=s]$\n",
        "\n"
      ],
      "metadata": {
        "id": "hwocbk__ZpeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "상태 가치 함수의 1번째 항:\n",
        "\n",
        "$ E_{\\pi}[R_t|S_t=s] = Σ_{a,s'} \\pi(a|s)\\>p(s'|s,a)\\>r(s,a,s')$\n",
        "\n",
        "에이전트가 선택하는 행동의 확률 * 전이되는 상태의 확률 * 그때의 보상\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "BO9AaebAk688"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "상태 가치 함수의 2번째 항:\n",
        "\n",
        "$ \\gamma E_{\\pi}[G_{t+1}|S_t=s] $\n",
        "\n",
        "상태 s에서 시간을 1만큼 흘렸다고 하면\n",
        "\n",
        "$ E_{\\pi}[G_{t+1}|S_t=s]\n",
        "\\\\= Σ_{a,s'} \\pi(a|s)\\>p(s'|s,a)\\> E_{\\pi}[G_{t+1} | S_{t+1} = s']\n",
        "\\\\= Σ_{a,s'} \\pi(a|s)\\>p(s'|s,a)\\> v_\\pi(s')$"
      ],
      "metadata": {
        "id": "viPd88Gon_as"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "최종 벨만 방정식:\n",
        "\n",
        "$v_\\pi(s) =  E_{\\pi}[R_t|S_t=s] + \\gamma E_{\\pi}[G_{t+1}|S_t=s]\n",
        "\\\\ = Σ_{a,s'} \\pi(a|s)\\>p(s'|s,a)\\>r(s,a,s') + \\gamma Σ_{a,s'} \\pi(a|s)\\>p(s'|s,a)\\> v_\\pi(s')\n",
        "\\\\ = Σ_{a,s'} \\pi(a|s)\\>p(s'|s,a)\\>\\{r(s,a,s') + \\gamma v_\\pi(s')\\}$"
      ],
      "metadata": {
        "id": "1w1UsVGRtnOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.2 벨만 방정식의 예"
      ],
      "metadata": {
        "id": "P5tK7Qs6XrBR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "그리드 월드 (p.90), 단 50% 확률로 왼쪽 이동하고 50% 확률로 오른쪽 이동함<br>\n",
        "\\\n",
        "\\\n",
        "결정적으로 상태가 전이될 시, 벨만 방정식:\n",
        "\n",
        "$v_\\pi(s) = Σ_{a} \\pi(a|s)\\>\\{r(s,a,s') + \\gamma v_\\pi(s')\\}$\n",
        "\n",
        "에이전트가 선택하는 행동의 확률 * {그때의 보상 + 할인율 * v(다음 상태)}\n",
        "\\\n",
        "\\\n",
        "L1에서 Left 선택 시 벨만 방정식:\n",
        "\n",
        "$0.5 * \\{-1 + 0.9 * v_\\pi(L1)\\}$\n",
        "\\\n",
        "\\\n",
        "L1에서 Right 선택 시 벨만 방정식:\n",
        "\n",
        "$0.5 * \\{1 + 0.9 * v_\\pi(L1)\\}$\n",
        "\\\n",
        "\\\n",
        "최종 L1에서의 방정식:\n",
        "\n",
        "$v_\\pi(L1) = 0.5\\{-1+0.9v_\\pi(L1)\\} + 0.5 \\{1+0.9v_\\pi(L2)\\}$\n",
        "$-0.55v_\\pi(L1) + 0.45v_\\pi(L2) = 0$\n",
        "\\\n",
        "\\\n",
        "최종 L2에서의 방정식:\n",
        "\n",
        "$0.45v_\\pi(L1) - 0.55v_\\pi(L2) = 0.5$\n",
        "\\\n",
        "\\\n",
        "결론:\n",
        "\n",
        "$v_\\pi(L1) = -2.25, v_\\pi(L2) = -2.75$"
      ],
      "metadata": {
        "id": "Y65pGSFcufYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.3 행동 가치 함수 (Q 함수)와 벨만 방정식"
      ],
      "metadata": {
        "id": "K_xfnEMnXu2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "상태 가치 함수: 상태 s에서 시작했을 때, 수익에 대한 기댓값\n",
        "\n",
        "$v_{\\pi}(s) = E_{\\pi}[G_t | S_t = s]$\n",
        "\n",
        "행동 가치 함수: 상태 가치 함수 + 행동 고려<br>\n",
        "(상태 가치 함수는 어떤 상태에서 **시작**하는 게 가치가 가장 높은지 알 수 있지만, 그 상태에서 어떤 **행동**을 취하는 게 가치가 가장 높은지 알 수 없음)\n",
        "\n",
        "$q_\\pi(s,a) = E_\\pi[G_t|S_t=s,A_t = a]$\n",
        "\n",
        "$q_\\pi(s,a)$에서 시간 t일 때 상태 s에서 **정책 $\\pi$와 무관하게** 행동 $a$를 선택, 다음 행동부터 정책 $\\pi$를 따름\n",
        "\n",
        "\\\n",
        "$v_{\\pi}(s) = \\Sigma_{a}\\>\\pi(a|s)\\>q_\\pi(s,a)$ (이유: p96)"
      ],
      "metadata": {
        "id": "enWKQ8-TZ6zk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "행동 가치 함수 + 벨만 방정식:\n",
        "\n",
        "$q_\\pi(s,a)\n",
        "\\\\= E_\\pi[G_t|S_t=s,A_t = a]\n",
        "\\\\= E_\\pi[R_t+\\gamma G_{t+1} | S_t = s, A_t = a]\n",
        "\\\\= \\Sigma_{s'}p(s'|s,a) \\{ r(s,a,s') + \\gamma v_\\pi(s')\\}\n",
        "\\\\= \\Sigma_{s'}p(s'|s,a) \\{ r(s,a,s') + \\gamma \\Sigma_{a'}\\pi(a'|s')q_\\pi(s',a') \\}\n",
        "$\n",
        "\n",
        "(a': 시간 t+1에서의 행동)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pSVGCmGDjSVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.4 벨만 최적 방정식"
      ],
      "metadata": {
        "id": "2Tn_IzFFXzvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "최적 정책: 모든 상태에서 상태 가치 함수가 최대인 정책<br>\n",
        "벨만 최적 방정식: 최적 정책에 대한 벨만 방정식\n",
        "\n",
        "$v_*(s)\n",
        "\\\\ = Σ_{a,s'} \\pi_*(a|s)\\>p(s'|s,a)\\>\\{r(s,a,s') + \\gamma v_*(s')\\}\n",
        "\\\\ = \\Sigma_a \\pi_*(a|s) \\> \\Sigma_{s'} p(s'|s,a)\\{r(s,a,s')+\\gamma v_*(s')\\}\n",
        "\\\\ = max_a \\Sigma_{s'}p(s'|s,a) \\{ r(s,a,s') + \\gamma v_*(s') \\}$\n",
        "\n",
        "\\\n",
        "\n",
        "최적 정책은 $\\Sigma_{s'} p(s'|s,a)\\{r(s,a,s')+\\gamma v_*(s')\\}$의 값이 가장 큰 행동 선택<br>\n",
        "그 최댓값은 그대로 $v_*(s)$<br>\n",
        "($\\pi_*(a|s)$ [에이전트가 선택하는 행동의 확률] 이 $\\Sigma_{s'} p(s'|s,a)\\{r(s,a,s')+\\gamma v_*(s')\\}$의 값이 최대면 1, 아니면 0이기 때문)\n",
        "\n",
        "$v_*(s) = max_a \\Sigma_{s'}p(s'|s,a) \\{ r(s,a,s') + \\gamma v_*(s') \\}$\n",
        "\n",
        "($max_a(f(a))$ - 입력:a, 출력: 모든 f(a) 중 최댓값)\n",
        "\n",
        "\\\n",
        "\n",
        "[예시]<br>\n",
        "$\\Sigma_{s'} p(s'|s,a)\\{r(s,a,s')+\\gamma v_*(s')\\}$의 값이,<br>\n",
        "$a = a_1$일 때 2,<br>\n",
        "$a = a_2$일 때 -1,<br>\n",
        "$a = a_3$일 때 4<br>\n",
        "-> $v_*(s) = 4$"
      ],
      "metadata": {
        "id": "i1UKICF7murM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "행동 가치 함수의 벨만 최적 방정식:\n",
        "\n",
        "$q_\\pi(s,a)= \\Sigma_{s'}p(s'|s,a) \\{ r(s,a,s') + \\gamma \\Sigma_{a'}\\pi(a'|s')q_\\pi(s',a') \\}$\n",
        "\n",
        "최적 정책에 대한 방정식 (최적 행동 가치 함수):\n",
        "\n",
        "$q_*(s,a)\n",
        "\\\\= \\Sigma_{s'}p(s'|s,a) \\{ r(s,a,s') + \\gamma \\Sigma_{a'}\\pi_*(a'|s')q_*(s',a') \\}\n",
        "\\\\= \\Sigma_{s'}p(s'|s,a) \\{ r(s,a,s') + \\gamma max_{a'}q_*(s',a') \\}$\n",
        "\n",
        "[?] 결정적 최적 정책(특정 상태에서 반드시 특정 행동 선택)이 하나 이상 존재하는 이유?"
      ],
      "metadata": {
        "id": "NRaOnBtKDJ7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.5 벨만 최적 방정식의 예"
      ],
      "metadata": {
        "id": "2XWbRjTyX2qv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$v_*(s) = max_a \\Sigma_{s'}p(s'|s,a) \\{ r(s,a,s') + \\gamma v_*(s') \\}$\n",
        "\n",
        "상태 전이가 결정적일 때,\n",
        "\n",
        "if $s' = f(s,a),\n",
        "\\\\v_*(s) = max_a \\{ r(s,a,s') + \\gamma v_*(s') \\}$\n",
        "\n",
        "$v_*(L1) = max \\{ -1 + 0.9v_*(L1), 1 + 0.9v_*(L2)\\}$\n",
        "\n",
        "$v_*(L2) = max \\{ 0.9v_*(L1), -1 + 0.9v_*(L2)\\}$\n",
        "\n",
        "$v_*(L1) = 5.26, v_*(L2) = 4.73$"
      ],
      "metadata": {
        "id": "BKVtg5eGUBAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "최적 행동 가치 함수 $q_*(s,a)$를 알고 있을 때, 상태 s에서의 최적 행동:\n",
        "\n",
        "$\\mu_*(s) = argmax_a \\>q_*(s,a)$\n",
        "\n",
        "$argmax(f(x))$ - 입력: x, 출력: f(x)를 최대로 만드는 x\n",
        "\n",
        "\\\\\n",
        "\n",
        "$q_\\pi(s,a) = \\Sigma_{s'}p(s'|s,a) \\{ r(s,a,s') + \\gamma v_\\pi(s')\\}$\n",
        "\n",
        "$\\mu_*(s) = argmax_a\\Sigma_{s'}p(s'|s,a) \\{ r(s,a,s') + \\gamma v_*(s')\\}$\n",
        "\n",
        "L1에서 Left 선택 시, $\\Sigma_{s'}p(s'|s,a) \\{ r(s,a,s') + \\gamma v_\\pi(s')\\}$ 부분의 값:\n",
        "\n",
        "$-1 + 0.9v_*(L1) = 3.734$\n",
        "\n",
        "L1에서 Right 선택 시, $\\Sigma_{s'}p(s'|s,a) \\{ r(s,a,s') + \\gamma v_\\pi(s')\\}$ 부분의 값:\n",
        "\n",
        "$-1 + 0.9v_*(L2) = 5.257$\n",
        "\n",
        "따라서 L1에서는 Right를 선택해야 함"
      ],
      "metadata": {
        "id": "Rv1bLY7mVgBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.6 정리"
      ],
      "metadata": {
        "id": "l02wyz80eawA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "벨만 방정식\n",
        "- 상태 가치 함수\n",
        " - $v_\\pi(s) = Σ_{a,s'} \\pi(a|s)\\>p(s'|s,a)\\>\\{r(s,a,s') + \\gamma v_\\pi(s')\\}$\n",
        " - 상태 s에서의 기대 수익\n",
        " - 현재 상태에서 출발했을 때, 앞으로 나오는 모든 경우의 수 (미래의 기대 가치까지) 를 취합하여 수익을 냄, 상태의 가치 도출\n",
        " - $\\pi(a|s)$ - 상태 s에서 행동 a를 취할 확률\n",
        " - $p(s'|s,a)$ - 상태 s에서 행동 a를 취했을 때, 상태 s'으로 넘어갈 확률\n",
        " - $r(s,a,s')$ - 상태 s에서 행동 a를 취한 뒤 s'으로 넘어갔을 때 얻는 보상\n",
        " - $\\gamma$ - 할인율 $(0 < \\gamma < 1)$\n",
        "- 행동 가치 함수\n",
        " - $q_π(s,a)=Σ_{s'}p(s'|s,a)\\{r(s,a,s')+γΣ_{a'}π(a'|s')q_π(s',a')\\}$\n",
        " - 상태 s에서 정책에 상관없이 행동 a를 취했을 때, 기대 수익\n",
        "\n",
        "벨만 최적 방정식\n",
        " - $v_*(s) = max_a \\Sigma_{s'}p(s'|s,a) \\{ r(s,a,s') + \\gamma v_*(s') \\}$\n",
        "  - 상태 s에서, 모든 행동 a를 취했을 때, 나오는 제일 높은 수익을 반환\n",
        " - $q_*(s,a) = \\Sigma_{s'}p(s'|s,a) \\{ r(s,a,s') + \\gamma max_{a'}q_*(s',a') \\}$\n",
        "\n",
        "최적 정책\n",
        " - $\\mu_*(s) = argmax_a \\>q_*(s,a)\n",
        " \\\\\n",
        "  \\qquad = argmax_a \\Sigma_{s'}p(s'|s,a) \\{ r(s,a,s') + \\gamma v_*(s')\\}$"
      ],
      "metadata": {
        "id": "LGUgsHjZeckM"
      }
    }
  ]
}